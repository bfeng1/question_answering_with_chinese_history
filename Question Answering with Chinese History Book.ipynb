{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9899332,"sourceType":"datasetVersion","datasetId":5926756}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/binfeng2021/question-answering-with-chinese-history-book?scriptVersionId=209598627\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# Main Goal\n\n* Process a pdf document of a Chinese hitory book, and use it as reference to answer fact-based questions related to Chinese history.\n* Embed the texts from the book and find the most similar text chunk as context to answer provided questions. \n* Build a questions answering bot that can process the information and find the most likely answer based on contexts.\n\n# Data \n[A History of CHINA by Morris Rossabi](https://arxiujosepserradell.cat/wp-content/uploads/2021/12/A-History-of-China-by-Morris-Rossabi-z-lib.org_.pdf)","metadata":{}},{"cell_type":"markdown","source":"## Install and import needed libraries","metadata":{}},{"cell_type":"code","source":"!pip install pymupdf","metadata":{"execution":{"iopub.status.busy":"2024-11-25T17:33:51.158291Z","iopub.execute_input":"2024-11-25T17:33:51.159325Z","iopub.status.idle":"2024-11-25T17:34:05.119536Z","shell.execute_reply.started":"2024-11-25T17:33:51.159286Z","shell.execute_reply":"2024-11-25T17:34:05.118173Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Collecting pymupdf\n  Downloading PyMuPDF-1.24.14-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (3.4 kB)\nDownloading PyMuPDF-1.24.14-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (19.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.8/19.8 MB\u001b[0m \u001b[31m58.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: pymupdf\nSuccessfully installed pymupdf-1.24.14\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!pip install sentence-transformers","metadata":{"execution":{"iopub.status.busy":"2024-11-25T17:34:05.122033Z","iopub.execute_input":"2024-11-25T17:34:05.122418Z","iopub.status.idle":"2024-11-25T17:34:15.796866Z","shell.execute_reply.started":"2024-11-25T17:34:05.122377Z","shell.execute_reply":"2024-11-25T17:34:15.795693Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Collecting sentence-transformers\n  Downloading sentence_transformers-3.3.1-py3-none-any.whl.metadata (10 kB)\nRequirement already satisfied: transformers<5.0.0,>=4.41.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (4.45.1)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (4.66.4)\nRequirement already satisfied: torch>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (2.4.0+cpu)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (1.2.2)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (1.14.1)\nRequirement already satisfied: huggingface-hub>=0.20.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (0.25.1)\nRequirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (10.3.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.15.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.6.1)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.4)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.5.15)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.4.5)\nRequirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.20.0)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (3.5.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface-hub>=0.20.0->sentence-transformers) (3.1.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2024.8.30)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\nDownloading sentence_transformers-3.3.1-py3-none-any.whl (268 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: sentence-transformers\nSuccessfully installed sentence-transformers-3.3.1\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import pymupdf\nimport re\nimport json\nimport pandas as pd\nimport math\n\nfrom sentence_transformers import SentenceTransformer, util\n\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport numpy as np\n\nfrom collections import Counter\n\nfrom fuzzywuzzy import fuzz\n\nimport nltk\nnltk.download('punkt')  # Download the sentence tokenizer\n\nfrom nltk.tokenize import sent_tokenize\nfrom transformers import pipeline\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"execution":{"iopub.status.busy":"2024-11-25T17:34:15.799107Z","iopub.execute_input":"2024-11-25T17:34:15.799488Z","iopub.status.idle":"2024-11-25T17:34:40.46308Z","shell.execute_reply.started":"2024-11-25T17:34:15.79945Z","shell.execute_reply":"2024-11-25T17:34:40.462071Z"},"trusted":true},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/fuzzywuzzy/fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n","output_type":"stream"},{"name":"stdout","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"def get_most_popular_item(l):\n    counter = Counter(l)\n    most_common = counter.most_common(1)\n    value, _ = most_common[0]\n    return value","metadata":{"execution":{"iopub.status.busy":"2024-11-25T17:34:40.466116Z","iopub.execute_input":"2024-11-25T17:34:40.467012Z","iopub.status.idle":"2024-11-25T17:34:40.472528Z","shell.execute_reply.started":"2024-11-25T17:34:40.466959Z","shell.execute_reply":"2024-11-25T17:34:40.471425Z"},"trusted":true},"outputs":[],"execution_count":4},{"cell_type":"code","source":"def extract_pdf_text_with_metadata(page_num, doc):\n    data = {'text':[], 'bbox':[], 'page':[], 'font':[], 'size':[]}\n    current_part = \"\"\n    min_pos = math.inf\n    page = doc.load_page(page_num)\n    \n    # extract text blocks with positional info\n    blocks = page.get_text(\"dict\")[\"blocks\"]\n    \n    for block in blocks:\n        if \"lines\" not in block:\n            continue\n        \n        bbox = block['bbox']\n        block_texts = \"\"\n        font_list = []\n        size_list = []\n        \n        for line in block[\"lines\"]:\n            line_texts = \"\"\n            for span in line[\"spans\"]:\n                line_texts += span[\"text\"].strip()\n                font_list.append(span[\"font\"])\n                size_list.append(span[\"size\"])\n                \n            if block_texts == \"\":\n                block_texts = line_texts\n            else:\n                block_texts = block_texts + \" \" + line_texts\n\n        # store text blocks with metadata\n        # first, choose the most popular font and size of the block to store\n        \n        data['font'].append(get_most_popular_item(font_list))\n        data['size'].append(get_most_popular_item(size_list))\n        data['text'].append(block_texts)\n        data['bbox'].append(bbox)\n        data['page'].append(page_num)\n    data = pd.DataFrame(data)\n    return data","metadata":{"execution":{"iopub.status.busy":"2024-11-25T17:34:40.4742Z","iopub.execute_input":"2024-11-25T17:34:40.474665Z","iopub.status.idle":"2024-11-25T17:34:40.495591Z","shell.execute_reply.started":"2024-11-25T17:34:40.474621Z","shell.execute_reply":"2024-11-25T17:34:40.494433Z"},"trusted":true},"outputs":[],"execution_count":5},{"cell_type":"code","source":"def dict_to_df(book_dict):\n    rows = []\n    for part, part_info in book_dict.items():\n        part_name = part_info[\"name\"]\n        for chapter, chapter_info in part_info[\"chapters\"].items():\n            chapter_name = chapter_info[\"name\"]\n            for section, pages in chapter_info[\"sections\"].items():\n                rows.append([part_name, chapter_name, section, pages])\n    \n    # Create a DataFrame from the list of rows\n    df = pd.DataFrame(rows, columns=[\"Part\", \"Chapter\", \"Section\", \"Pages\"])\n    return df","metadata":{"execution":{"iopub.status.busy":"2024-11-25T17:34:40.497498Z","iopub.execute_input":"2024-11-25T17:34:40.498038Z","iopub.status.idle":"2024-11-25T17:34:40.51092Z","shell.execute_reply.started":"2024-11-25T17:34:40.497991Z","shell.execute_reply":"2024-11-25T17:34:40.509788Z"},"trusted":true},"outputs":[],"execution_count":6},{"cell_type":"code","source":"def find_most_similar(text, df, model, n = 5):\n    # Encode the user query\n    query_embedding = model.encode(text)\n    \n    # Calculate cosine similarities\n    similarities = cosine_similarity([query_embedding], df['embeddings'].tolist())\n    \n    # Find the index of the most similar text\n    # most_similar_idx = similarities.argmax()\n    top_n_indices = np.argsort(similarities[0])[-n:][::-1]\n    \n    # Return the relevant info (text, page number, part, and chapter)\n    relevant_info = df_process_pdf.iloc[top_n_indices]\n    relevant_info.reset_index(drop = True, inplace = True)\n    relevant_info['relevant_rank'] = relevant_info.index + 1\n    return relevant_info","metadata":{"execution":{"iopub.status.busy":"2024-11-25T17:34:40.512638Z","iopub.execute_input":"2024-11-25T17:34:40.512958Z","iopub.status.idle":"2024-11-25T17:34:40.53019Z","shell.execute_reply.started":"2024-11-25T17:34:40.512925Z","shell.execute_reply":"2024-11-25T17:34:40.528878Z"},"trusted":true},"outputs":[],"execution_count":7},{"cell_type":"code","source":"def get_context(context_info, context_window = 10):\n    context_list = []\n    relevant_rank_list = []\n    for _, row in context_info.iterrows():\n        sen_num = row['sen_num']\n        part = row['Part']\n        chapter = row['Chapter']\n        section = row['Section']\n        relevant_rank = row['relevant_rank']\n        sentences_list = df_process_pdf[(df_process_pdf['Part']==part)\n        &(df_process_pdf['Chapter']==chapter)\n        &(df_process_pdf['Section']==section)\n        &(df_process_pdf['sen_num']>=sen_num - context_window)\n        &(df_process_pdf['sen_num']<=sen_num + context_window)].sentences.tolist()\n        context = ' '.join(sentences_list)\n        context_list.append(context)\n        relevant_rank_list.append(relevant_rank)\n    return context_list, relevant_rank_list","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T17:34:40.53199Z","iopub.execute_input":"2024-11-25T17:34:40.532414Z","iopub.status.idle":"2024-11-25T17:34:40.543923Z","shell.execute_reply.started":"2024-11-25T17:34:40.532375Z","shell.execute_reply":"2024-11-25T17:34:40.542693Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"def generate_similarity_score(row):\n    similarity = util.cos_sim(row['gt_embeddings'], row['predict_embeddings'])\n    return similarity.item()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T17:34:40.545177Z","iopub.execute_input":"2024-11-25T17:34:40.545517Z","iopub.status.idle":"2024-11-25T17:34:40.557456Z","shell.execute_reply.started":"2024-11-25T17:34:40.545487Z","shell.execute_reply":"2024-11-25T17:34:40.55639Z"}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"## Process the PDF documents","metadata":{}},{"cell_type":"code","source":"doc = pymupdf.open('/kaggle/input/a-history-of-china-morris-rossabi/A-History-of-China-by-Morris-Rossabi-z-lib.org_.pdf')\n\ndf_full_doc = []\nfor page_n in range(doc.page_count):\n    processed_df = extract_pdf_text_with_metadata(page_n, doc)\n    processed_df['page_num'] = page_n + 1\n    df_full_doc.append(processed_df)\n\ndf_full_doc = pd.concat(df_full_doc)\n\ndf_full_doc['y0'] = df_full_doc['bbox'].apply(lambda x: x[1])\ndf_full_doc = df_full_doc.groupby('page_num', as_index=False).apply(lambda x: x.sort_values('y0').reset_index(drop = True).reset_index())\ndf_full_doc['block_num'] = df_full_doc['index'] + 1\n\ndf_full_doc = df_full_doc[(df_full_doc['font'].isin(['PlantinStd', 'PlantinStd-Italic-SC700']))&(df_full_doc['size'].round(2)>=10)&(df_full_doc['page_num']>=31)&(df_full_doc['page_num']<441)]\n\ndf_full_doc.loc[(df_full_doc['font']=='PlantinStd-Italic-SC700')&(df_full_doc['size'].round(2)==19.47), 'text_function'] = 'section title'\n\ndf_full_doc.loc[(df_full_doc['font']=='PlantinStd')&(df_full_doc['text_function'].isnull())&(~df_full_doc['text'].str.startswith('Figure'))&(~df_full_doc['text'].str.startswith('Map')), 'text_function'] = 'main body'\n\ndf_in = df_full_doc[df_full_doc['text_function'].isin(['main body', 'section title'])][['page_num', 'block_num', 'text', 'text_function']].drop_duplicates().sort_values(['page_num', 'block_num']).reset_index(drop = True)","metadata":{"execution":{"iopub.status.busy":"2024-11-25T17:34:40.560971Z","iopub.execute_input":"2024-11-25T17:34:40.561347Z","iopub.status.idle":"2024-11-25T17:34:43.231288Z","shell.execute_reply.started":"2024-11-25T17:34:40.561315Z","shell.execute_reply":"2024-11-25T17:34:43.230398Z"},"trusted":true},"outputs":[],"execution_count":10},{"cell_type":"code","source":"section_list = df_in[df_in['text_function']=='section title'].index.tolist() + [math.inf]","metadata":{"execution":{"iopub.status.busy":"2024-11-25T17:34:43.232357Z","iopub.execute_input":"2024-11-25T17:34:43.232647Z","iopub.status.idle":"2024-11-25T17:34:43.239795Z","shell.execute_reply.started":"2024-11-25T17:34:43.232619Z","shell.execute_reply":"2024-11-25T17:34:43.23859Z"},"trusted":true},"outputs":[],"execution_count":11},{"cell_type":"code","source":"dict_ref = {'section':[], 'text':[]}\nfor (start, end) in zip(section_list, section_list[1:]):\n    df = df_in[(df_in.index >start)&(df_in.index <end)]\n    section = df_in[df_in.index==start].text.values[0]\n    dict_ref['section'].append(section)\n    text_all = \" \".join(df['text'].unique().tolist())\n    dict_ref['text'].append(text_all)\n\ndict_ref = pd.DataFrame(dict_ref)\n\n# Remove further reading and notes sections\ndict_ref = dict_ref[~dict_ref['section'].isin(['FURTHERREADING', 'NOTES'])]","metadata":{"execution":{"iopub.status.busy":"2024-11-25T17:34:43.241252Z","iopub.execute_input":"2024-11-25T17:34:43.241704Z","iopub.status.idle":"2024-11-25T17:34:43.358258Z","shell.execute_reply.started":"2024-11-25T17:34:43.241661Z","shell.execute_reply":"2024-11-25T17:34:43.356742Z"},"trusted":true},"outputs":[],"execution_count":12},{"cell_type":"code","source":"with open('/kaggle/input/a-history-of-china-morris-rossabi/table of contents.txt') as file:\n    content = file.read()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T17:34:43.359709Z","iopub.execute_input":"2024-11-25T17:34:43.360144Z","iopub.status.idle":"2024-11-25T17:34:43.367702Z","shell.execute_reply.started":"2024-11-25T17:34:43.360099Z","shell.execute_reply":"2024-11-25T17:34:43.366655Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"# Convert part of the dictionary to DataFrame\ndf_structure = dict_to_df(json.loads(content))\n\n# note that the page number we will be using is the page of the pdf document, not the book. There are a difference of 28 between book page and pdf page\ndf_structure['book_page'] = df_structure['Pages'].str.extract('(\\d+)')\ndf_structure['page_start'] = df_structure['book_page'].astype(float) + 28 # offset the page differences, so we will talk about pdf pages going forward\ndf_structure['page_end'] = df_structure['page_start'].shift(-1)\n\ndf_structure.loc[df_structure['page_end'].isnull(), 'page_end'] = df_structure.loc[df_structure['page_end'].isnull(), 'page_start']","metadata":{"execution":{"iopub.status.busy":"2024-11-25T17:34:43.369145Z","iopub.execute_input":"2024-11-25T17:34:43.369522Z","iopub.status.idle":"2024-11-25T17:34:43.385332Z","shell.execute_reply.started":"2024-11-25T17:34:43.369489Z","shell.execute_reply":"2024-11-25T17:34:43.384107Z"},"trusted":true},"outputs":[],"execution_count":14},{"cell_type":"code","source":"extracted_sections = dict_ref['section'].unique().tolist()\noriginal_sections = df_structure['Section'].unique().tolist()\n\nsection_match = {'section':[], 'Section':[], 'ratio':[]}\nfor l1 in extracted_sections:\n    l1_ = l1.replace(' ', '')\n    l1_ = l1_.replace(r'[^\\w\\s]', '')\n    for l2 in original_sections:\n        l2_ = l2.upper()\n        l2_ = l2_.replace(' ', '')\n        l2_ = l2_.replace(r'[^\\w\\s]', '')\n        ratio = fuzz.ratio(l1_, l2_)\n        section_match['section'].append(l1)\n        section_match['Section'].append(l2)\n        section_match['ratio'].append(ratio)\n\nsection_match = pd.DataFrame(section_match).sort_values('ratio', ascending=False).groupby(['section'], as_index=False).head(1).drop('ratio', axis = 1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T17:34:43.386837Z","iopub.execute_input":"2024-11-25T17:34:43.38718Z","iopub.status.idle":"2024-11-25T17:34:44.552781Z","shell.execute_reply.started":"2024-11-25T17:34:43.387147Z","shell.execute_reply":"2024-11-25T17:34:44.551687Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"df_process_pdf = df_structure.merge(section_match).merge(dict_ref).drop('section', axis = 1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T17:34:44.55395Z","iopub.execute_input":"2024-11-25T17:34:44.554292Z","iopub.status.idle":"2024-11-25T17:34:44.573539Z","shell.execute_reply.started":"2024-11-25T17:34:44.554255Z","shell.execute_reply":"2024-11-25T17:34:44.572305Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"df_process_pdf['sentences'] = df_process_pdf['text'].apply(lambda x: sent_tokenize(x))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T17:34:44.575296Z","iopub.execute_input":"2024-11-25T17:34:44.575812Z","iopub.status.idle":"2024-11-25T17:34:44.897034Z","shell.execute_reply.started":"2024-11-25T17:34:44.575761Z","shell.execute_reply":"2024-11-25T17:34:44.895976Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"df_process_pdf = df_process_pdf.explode('sentences').reset_index(drop = True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T17:34:44.898562Z","iopub.execute_input":"2024-11-25T17:34:44.899011Z","iopub.status.idle":"2024-11-25T17:34:44.916409Z","shell.execute_reply.started":"2024-11-25T17:34:44.898959Z","shell.execute_reply":"2024-11-25T17:34:44.915514Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"# build text embeddings\nembeddings_model = SentenceTransformer(\"all-mpnet-base-v2\")","metadata":{"execution":{"iopub.status.busy":"2024-11-25T17:34:44.918195Z","iopub.execute_input":"2024-11-25T17:34:44.918732Z","iopub.status.idle":"2024-11-25T17:34:51.163903Z","shell.execute_reply.started":"2024-11-25T17:34:44.918693Z","shell.execute_reply":"2024-11-25T17:34:51.16268Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7fb032196c2b423bbeab4435cb0e3e4d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9437ab4f03d44f7981d8d21c653eeba3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/10.6k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a4b9a3cea3284a80a7d4a09d7a530696"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f474ffa26e6e4596a4a5ff68b883e331"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"14ba883c3bc14ed4b6f4232f45f85118"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ab2953db535d46a78adc67ced0fdc426"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"70f56fbcd3f349b59d7ab042f3a03848"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"23a6b1c360a044daba20122e2244773a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c999cf69f88a4b00aa1957be4a75d67e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"703c21534c8247d1bedaa3f0d23cd69e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b24a6e750bc24aa88994fda5a507ccbd"}},"metadata":{}}],"execution_count":19},{"cell_type":"code","source":"df_process_pdf['embeddings'] = df_process_pdf['sentences'].apply(lambda x: embeddings_model.encode(x, show_progress_bar = False))","metadata":{"execution":{"iopub.status.busy":"2024-11-25T17:34:51.165316Z","iopub.execute_input":"2024-11-25T17:34:51.165697Z","iopub.status.idle":"2024-11-25T17:47:24.586335Z","shell.execute_reply.started":"2024-11-25T17:34:51.165663Z","shell.execute_reply":"2024-11-25T17:47:24.584991Z"},"trusted":true},"outputs":[],"execution_count":20},{"cell_type":"code","source":"df_process_pdf = df_process_pdf.groupby(['Part', 'Chapter', 'Section'], as_index=False).apply(lambda x: x.reset_index(drop = True).reset_index())\n\ndf_process_pdf.rename(columns = {'index':'sen_num'}, inplace = True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T17:47:24.587914Z","iopub.execute_input":"2024-11-25T17:47:24.588332Z","iopub.status.idle":"2024-11-25T17:47:24.687473Z","shell.execute_reply.started":"2024-11-25T17:47:24.588297Z","shell.execute_reply":"2024-11-25T17:47:24.68625Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"df_process_pdf.to_excel('processed_book.xlsx', index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T18:37:40.867878Z","iopub.execute_input":"2024-11-25T18:37:40.868308Z","iopub.status.idle":"2024-11-25T18:38:38.073769Z","shell.execute_reply.started":"2024-11-25T18:37:40.868272Z","shell.execute_reply":"2024-11-25T18:38:38.072638Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"df_process_pdf","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T18:37:21.004091Z","iopub.execute_input":"2024-11-25T18:37:21.004545Z","iopub.status.idle":"2024-11-25T18:37:21.037285Z","shell.execute_reply.started":"2024-11-25T18:37:21.004508Z","shell.execute_reply":"2024-11-25T18:37:21.036078Z"}},"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"        sen_num                      Part  \\\n0   0         0  China Among 'Barbarians'   \n    1         1  China Among 'Barbarians'   \n    2         2  China Among 'Barbarians'   \n    3         3  China Among 'Barbarians'   \n    4         4  China Among 'Barbarians'   \n...         ...                       ...   \n134 56       56   China in Global History   \n    57       57   China in Global History   \n    58       58   China in Global History   \n    59       59   China in Global History   \n    60       60   China in Global History   \n\n                                                  Chapter  \\\n0   0   Chaos and Religious and Political Responses, 2...   \n    1   Chaos and Religious and Political Responses, 2...   \n    2   Chaos and Religious and Political Responses, 2...   \n    3   Chaos and Religious and Political Responses, 2...   \n    4   Chaos and Religious and Political Responses, 2...   \n...                                                   ...   \n134 56                   The Republican Period, 1911–1949   \n    57                   The Republican Period, 1911–1949   \n    58                   The Republican Period, 1911–1949   \n    59                   The Republican Period, 1911–1949   \n    60                   The Republican Period, 1911–1949   \n\n                      Section      Pages book_page  page_start  page_end  \\\n0   0   Buddhism Enters China  Pages 110       110       138.0     144.0   \n    1   Buddhism Enters China  Pages 110       110       138.0     144.0   \n    2   Buddhism Enters China  Pages 110       110       138.0     144.0   \n    3   Buddhism Enters China  Pages 110       110       138.0     144.0   \n    4   Buddhism Enters China  Pages 110       110       138.0     144.0   \n...                       ...        ...       ...         ...       ...   \n134 56      Warlords in Power  Pages 337       337       365.0     368.0   \n    57      Warlords in Power  Pages 337       337       365.0     368.0   \n    58      Warlords in Power  Pages 337       337       365.0     368.0   \n    59      Warlords in Power  Pages 337       337       365.0     368.0   \n    60      Warlords in Power  Pages 337       337       365.0     368.0   \n\n                                                     text  \\\n0   0   Buddhism is linked with the Indian prince Sidd...   \n    1   Buddhism is linked with the Indian prince Sidd...   \n    2   Buddhism is linked with the Indian prince Sidd...   \n    3   Buddhism is linked with the Indian prince Sidd...   \n    4   Buddhism is linked with the Indian prince Sidd...   \n...                                                   ...   \n134 56  The northwest province of Xinjiang illustrates...   \n    57  The northwest province of Xinjiang illustrates...   \n    58  The northwest province of Xinjiang illustrates...   \n    59  The northwest province of Xinjiang illustrates...   \n    60  The northwest province of Xinjiang illustrates...   \n\n                                                sentences  \\\n0   0   Buddhism is linked with the Indian prince Sidd...   \n    1   Because descriptions of his life evolved centu...   \n    2   Such writings are not reliable accounts, as th...   \n    3   Actual events are so interlaced with myths and...   \n    4   Yet  consideration of the mythical accounts co...   \n...                                                   ...   \n134 56  Some in the old gentry class profited from the...   \n    57  Virtually without restrictions, these landlord...   \n    58  Alliance with local officials, who either deri...   \n    59  Chinese who believed themselves to be exploite...   \n    60  They became increasingly frustrated with gover...   \n\n                                               embeddings  \n0   0   [0.038585927, 0.020595705, -0.035208672, 0.007...  \n    1   [0.021002242, 0.016055271, -0.04977115, 0.0365...  \n    2   [0.086978056, 0.028460192, -0.01349521, 0.0572...  \n    3   [0.06389172, 0.051194068, -0.015496978, 0.0812...  \n    4   [0.030101553, 0.05770132, -0.032306902, 0.0712...  \n...                                                   ...  \n134 56  [-0.02893394, 0.05812279, 0.03695344, 0.080495...  \n    57  [-0.045782655, 0.038755514, -0.023722423, -0.0...  \n    58  [-0.07431569, 0.055751823, 0.019721208, 0.0838...  \n    59  [-0.020467656, 0.022244317, 0.048469592, -0.01...  \n    60  [-0.027096061, 0.033532456, 0.03127649, -0.024...  \n\n[8140 rows x 11 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th></th>\n      <th>sen_num</th>\n      <th>Part</th>\n      <th>Chapter</th>\n      <th>Section</th>\n      <th>Pages</th>\n      <th>book_page</th>\n      <th>page_start</th>\n      <th>page_end</th>\n      <th>text</th>\n      <th>sentences</th>\n      <th>embeddings</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th rowspan=\"5\" valign=\"top\">0</th>\n      <th>0</th>\n      <td>0</td>\n      <td>China Among 'Barbarians'</td>\n      <td>Chaos and Religious and Political Responses, 2...</td>\n      <td>Buddhism Enters China</td>\n      <td>Pages 110</td>\n      <td>110</td>\n      <td>138.0</td>\n      <td>144.0</td>\n      <td>Buddhism is linked with the Indian prince Sidd...</td>\n      <td>Buddhism is linked with the Indian prince Sidd...</td>\n      <td>[0.038585927, 0.020595705, -0.035208672, 0.007...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>China Among 'Barbarians'</td>\n      <td>Chaos and Religious and Political Responses, 2...</td>\n      <td>Buddhism Enters China</td>\n      <td>Pages 110</td>\n      <td>110</td>\n      <td>138.0</td>\n      <td>144.0</td>\n      <td>Buddhism is linked with the Indian prince Sidd...</td>\n      <td>Because descriptions of his life evolved centu...</td>\n      <td>[0.021002242, 0.016055271, -0.04977115, 0.0365...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>China Among 'Barbarians'</td>\n      <td>Chaos and Religious and Political Responses, 2...</td>\n      <td>Buddhism Enters China</td>\n      <td>Pages 110</td>\n      <td>110</td>\n      <td>138.0</td>\n      <td>144.0</td>\n      <td>Buddhism is linked with the Indian prince Sidd...</td>\n      <td>Such writings are not reliable accounts, as th...</td>\n      <td>[0.086978056, 0.028460192, -0.01349521, 0.0572...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>China Among 'Barbarians'</td>\n      <td>Chaos and Religious and Political Responses, 2...</td>\n      <td>Buddhism Enters China</td>\n      <td>Pages 110</td>\n      <td>110</td>\n      <td>138.0</td>\n      <td>144.0</td>\n      <td>Buddhism is linked with the Indian prince Sidd...</td>\n      <td>Actual events are so interlaced with myths and...</td>\n      <td>[0.06389172, 0.051194068, -0.015496978, 0.0812...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>China Among 'Barbarians'</td>\n      <td>Chaos and Religious and Political Responses, 2...</td>\n      <td>Buddhism Enters China</td>\n      <td>Pages 110</td>\n      <td>110</td>\n      <td>138.0</td>\n      <td>144.0</td>\n      <td>Buddhism is linked with the Indian prince Sidd...</td>\n      <td>Yet  consideration of the mythical accounts co...</td>\n      <td>[0.030101553, 0.05770132, -0.032306902, 0.0712...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th rowspan=\"5\" valign=\"top\">134</th>\n      <th>56</th>\n      <td>56</td>\n      <td>China in Global History</td>\n      <td>The Republican Period, 1911–1949</td>\n      <td>Warlords in Power</td>\n      <td>Pages 337</td>\n      <td>337</td>\n      <td>365.0</td>\n      <td>368.0</td>\n      <td>The northwest province of Xinjiang illustrates...</td>\n      <td>Some in the old gentry class profited from the...</td>\n      <td>[-0.02893394, 0.05812279, 0.03695344, 0.080495...</td>\n    </tr>\n    <tr>\n      <th>57</th>\n      <td>57</td>\n      <td>China in Global History</td>\n      <td>The Republican Period, 1911–1949</td>\n      <td>Warlords in Power</td>\n      <td>Pages 337</td>\n      <td>337</td>\n      <td>365.0</td>\n      <td>368.0</td>\n      <td>The northwest province of Xinjiang illustrates...</td>\n      <td>Virtually without restrictions, these landlord...</td>\n      <td>[-0.045782655, 0.038755514, -0.023722423, -0.0...</td>\n    </tr>\n    <tr>\n      <th>58</th>\n      <td>58</td>\n      <td>China in Global History</td>\n      <td>The Republican Period, 1911–1949</td>\n      <td>Warlords in Power</td>\n      <td>Pages 337</td>\n      <td>337</td>\n      <td>365.0</td>\n      <td>368.0</td>\n      <td>The northwest province of Xinjiang illustrates...</td>\n      <td>Alliance with local officials, who either deri...</td>\n      <td>[-0.07431569, 0.055751823, 0.019721208, 0.0838...</td>\n    </tr>\n    <tr>\n      <th>59</th>\n      <td>59</td>\n      <td>China in Global History</td>\n      <td>The Republican Period, 1911–1949</td>\n      <td>Warlords in Power</td>\n      <td>Pages 337</td>\n      <td>337</td>\n      <td>365.0</td>\n      <td>368.0</td>\n      <td>The northwest province of Xinjiang illustrates...</td>\n      <td>Chinese who believed themselves to be exploite...</td>\n      <td>[-0.020467656, 0.022244317, 0.048469592, -0.01...</td>\n    </tr>\n    <tr>\n      <th>60</th>\n      <td>60</td>\n      <td>China in Global History</td>\n      <td>The Republican Period, 1911–1949</td>\n      <td>Warlords in Power</td>\n      <td>Pages 337</td>\n      <td>337</td>\n      <td>365.0</td>\n      <td>368.0</td>\n      <td>The northwest province of Xinjiang illustrates...</td>\n      <td>They became increasingly frustrated with gover...</td>\n      <td>[-0.027096061, 0.033532456, 0.03127649, -0.024...</td>\n    </tr>\n  </tbody>\n</table>\n<p>8140 rows × 11 columns</p>\n</div>"},"metadata":{}}],"execution_count":25},{"cell_type":"markdown","source":"## Build QA bot to answer questions","metadata":{}},{"cell_type":"code","source":"qa_pipeline = pipeline(\"question-answering\", model = \"distilbert-base-uncased-distilled-squad\")","metadata":{"execution":{"iopub.status.busy":"2024-11-25T17:47:24.995438Z","iopub.status.idle":"2024-11-25T17:47:24.996401Z","shell.execute_reply.started":"2024-11-25T17:47:24.996032Z","shell.execute_reply":"2024-11-25T17:47:24.996067Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_questions = pd.read_excel(\"/kaggle/input/a-history-of-china-morris-rossabi/QA_dataset.xlsx\", sheet_name = 'Sheet1')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T17:47:24.998385Z","iopub.status.idle":"2024-11-25T17:47:24.998886Z","shell.execute_reply.started":"2024-11-25T17:47:24.998686Z","shell.execute_reply":"2024-11-25T17:47:24.998708Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_results = []\nc = 0\nfor row, df_row in df_questions.iterrows():\n    question = df_row.Question\n    context_info = find_most_similar(question, df_process_pdf, embeddings_model)\n    context_list, relevant_rank_list = get_context(context_info)\n    # Get the answer\n    for context, relevant_rank in zip(context_list, relevant_rank_list):\n        result = qa_pipeline(question=question, context=context)\n        df_row['generated_answer'] = result['answer']\n        df_row['score'] = result['score']\n        df_row['relevant_rank'] = relevant_rank\n        df_results.append(pd.DataFrame(df_row).T)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T17:47:24.999886Z","iopub.status.idle":"2024-11-25T17:47:25.000272Z","shell.execute_reply.started":"2024-11-25T17:47:25.00007Z","shell.execute_reply":"2024-11-25T17:47:25.000087Z"},"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_results = pd.concat(df_results)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T17:47:25.002685Z","iopub.status.idle":"2024-11-25T17:47:25.003253Z","shell.execute_reply.started":"2024-11-25T17:47:25.002972Z","shell.execute_reply":"2024-11-25T17:47:25.003Z"},"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Evaluate the Results Generated From the Model\n\nThere are two approaches that I created to generate the answers:\n\n1. Use the texts with highest rank as the context to feed to the QA model, and generate the answer;\n2. Use the top 5 highest rank texts as the contexts, and generate the answer for each context. Suming the scores up for each generarted answer and provide the answer with highest total scoring as the final answer.","metadata":{}},{"cell_type":"code","source":"# first method, we will only use the answer generated from the highest rank context\ndf_results1 = df_results[df_results['relevant_rank']==1]\n\ndf_results1.sort_values('score', ascending=False).head(10)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T17:47:25.004344Z","iopub.status.idle":"2024-11-25T17:47:25.004708Z","shell.execute_reply.started":"2024-11-25T17:47:25.004535Z","shell.execute_reply":"2024-11-25T17:47:25.004553Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# second method, we will calculate the total score for each generated answers from the top 5 contexts, then choose the answer with highest score\ndf_results2 = df_results.groupby(['Question', 'Answer', 'generated_answer'], as_index=False)['score'].sum()\ndf_results2 = df_results2.sort_values('score', ascending=False).groupby(['Question', 'Answer'], as_index=False).head(1)\ndf_results2.sort_values('score', ascending=False).head(10)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T17:47:25.006683Z","iopub.status.idle":"2024-11-25T17:47:25.007099Z","shell.execute_reply.started":"2024-11-25T17:47:25.006921Z","shell.execute_reply":"2024-11-25T17:47:25.00694Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Use semantic similarity to evaluate the generated answers against the provided answers","metadata":{"execution":{"iopub.status.busy":"2024-11-16T15:18:06.983255Z","iopub.execute_input":"2024-11-16T15:18:06.983772Z","iopub.status.idle":"2024-11-16T15:18:06.988976Z","shell.execute_reply.started":"2024-11-16T15:18:06.983726Z","shell.execute_reply":"2024-11-16T15:18:06.98764Z"}}},{"cell_type":"code","source":"# using sentence transformer to embed the generated answers and the ground truth, so we can calculate the semantic similarity\ndf_results1['gt_embeddings'] = df_results1['Answer'].apply(lambda x: embeddings_model.encode(x, show_progress_bar = False))\ndf_results1['predict_embeddings'] = df_results1['generated_answer'].apply(lambda x: embeddings_model.encode(x, show_progress_bar = False))\ndf_results1['similarity'] = df_results1.apply(generate_similarity_score, axis = 1)\n\n# calculate similarity for results 2\ndf_results2['gt_embeddings'] = df_results2['Answer'].apply(lambda x: embeddings_model.encode(x, show_progress_bar = False))\ndf_results2['predict_embeddings'] = df_results2['generated_answer'].apply(lambda x: embeddings_model.encode(x, show_progress_bar = False))\ndf_results2['similarity'] = df_results2.apply(generate_similarity_score, axis = 1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T17:47:25.008489Z","iopub.status.idle":"2024-11-25T17:47:25.008969Z","shell.execute_reply.started":"2024-11-25T17:47:25.008764Z","shell.execute_reply":"2024-11-25T17:47:25.008789Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_results1[df_results1['similarity']>=0.8].shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T17:47:25.010841Z","iopub.status.idle":"2024-11-25T17:47:25.011247Z","shell.execute_reply.started":"2024-11-25T17:47:25.011043Z","shell.execute_reply":"2024-11-25T17:47:25.011061Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_results2[df_results2['similarity']>=0.8].shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T17:47:25.012352Z","iopub.status.idle":"2024-11-25T17:47:25.0127Z","shell.execute_reply.started":"2024-11-25T17:47:25.01252Z","shell.execute_reply":"2024-11-25T17:47:25.012536Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Future Work\n\n* Improve the accuracy of the context extraction. Currently I comparied the similarity of provided question and each senteces of the book to find the most similar sentence. Then I expand to a block of texts based on window size. However, based on how the question is phrased, sometimes it could lead to the incorrect contexts.\n\n* The question answering model is pre-trained model, the results generated might not be the most optimal yet. However, the proper dataset is lacking currently to finetune the model. ","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}